{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from abandabot.evaluate import evaluate_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reports/performance.json\") as f:\n",
    "    perf = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>context</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>no+context+no+reasoning</td>\n",
       "      <td>0.668±nan</td>\n",
       "      <td>0.585±nan</td>\n",
       "      <td>0.582±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>no+context</td>\n",
       "      <td>0.712±nan</td>\n",
       "      <td>0.625±nan</td>\n",
       "      <td>0.634±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>no+reasoning</td>\n",
       "      <td>0.720±nan</td>\n",
       "      <td>0.737±nan</td>\n",
       "      <td>0.727±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>context+reasoning</td>\n",
       "      <td>0.699±nan</td>\n",
       "      <td>0.699±nan</td>\n",
       "      <td>0.699±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>context+reasoning+complex</td>\n",
       "      <td>0.775±nan</td>\n",
       "      <td>0.807±nan</td>\n",
       "      <td>0.784±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>no+context+no+reasoning</td>\n",
       "      <td>0.747±nan</td>\n",
       "      <td>0.602±nan</td>\n",
       "      <td>0.602±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>no+context</td>\n",
       "      <td>0.657±nan</td>\n",
       "      <td>0.542±nan</td>\n",
       "      <td>0.510±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>no+reasoning</td>\n",
       "      <td>0.785±nan</td>\n",
       "      <td>0.591±nan</td>\n",
       "      <td>0.582±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>context+reasoning</td>\n",
       "      <td>0.691±nan</td>\n",
       "      <td>0.605±nan</td>\n",
       "      <td>0.609±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>context+reasoning+complex</td>\n",
       "      <td>0.677±nan</td>\n",
       "      <td>0.574±nan</td>\n",
       "      <td>0.564±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>no+context+no+reasoning</td>\n",
       "      <td>0.766±nan</td>\n",
       "      <td>0.622±nan</td>\n",
       "      <td>0.629±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>no+context</td>\n",
       "      <td>0.772±nan</td>\n",
       "      <td>0.598±nan</td>\n",
       "      <td>0.584±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>no+reasoning</td>\n",
       "      <td>0.679±nan</td>\n",
       "      <td>0.709±nan</td>\n",
       "      <td>0.651±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>context+reasoning</td>\n",
       "      <td>0.700±nan</td>\n",
       "      <td>0.736±nan</td>\n",
       "      <td>0.685±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>context+reasoning+complex</td>\n",
       "      <td>0.715±nan</td>\n",
       "      <td>0.753±nan</td>\n",
       "      <td>0.707±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama-v3p3</td>\n",
       "      <td>no+context+no+reasoning</td>\n",
       "      <td>0.348±nan</td>\n",
       "      <td>0.500±nan</td>\n",
       "      <td>0.410±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>llama-v3p3</td>\n",
       "      <td>no+context</td>\n",
       "      <td>0.856±nan</td>\n",
       "      <td>0.540±nan</td>\n",
       "      <td>0.490±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>llama-v3p3</td>\n",
       "      <td>no+reasoning</td>\n",
       "      <td>0.663±nan</td>\n",
       "      <td>0.596±nan</td>\n",
       "      <td>0.599±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>llama-v3p3</td>\n",
       "      <td>context+reasoning</td>\n",
       "      <td>0.674±nan</td>\n",
       "      <td>0.705±nan</td>\n",
       "      <td>0.667±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>llama-v3p3</td>\n",
       "      <td>context+reasoning+complex</td>\n",
       "      <td>0.755±nan</td>\n",
       "      <td>0.761±nan</td>\n",
       "      <td>0.758±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gemini-2.0</td>\n",
       "      <td>no+context+no+reasoning</td>\n",
       "      <td>0.348±nan</td>\n",
       "      <td>0.500±nan</td>\n",
       "      <td>0.410±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gemini-2.0</td>\n",
       "      <td>no+context</td>\n",
       "      <td>0.348±nan</td>\n",
       "      <td>0.500±nan</td>\n",
       "      <td>0.410±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gemini-2.0</td>\n",
       "      <td>no+reasoning</td>\n",
       "      <td>0.861±nan</td>\n",
       "      <td>0.560±nan</td>\n",
       "      <td>0.526±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gemini-2.0</td>\n",
       "      <td>context+reasoning</td>\n",
       "      <td>0.621±nan</td>\n",
       "      <td>0.590±nan</td>\n",
       "      <td>0.593±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gemini-2.0</td>\n",
       "      <td>context+reasoning+complex</td>\n",
       "      <td>0.766±nan</td>\n",
       "      <td>0.622±nan</td>\n",
       "      <td>0.629±nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>yesman</td>\n",
       "      <td>yesman</td>\n",
       "      <td>0.347561</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.410072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>noman</td>\n",
       "      <td>noman</td>\n",
       "      <td>0.152439</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.233645</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>random</td>\n",
       "      <td>random</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model                    context macro_precision macro_recall  \\\n",
       "0        gpt-4o    no+context+no+reasoning       0.668±nan    0.585±nan   \n",
       "1        gpt-4o                 no+context       0.712±nan    0.625±nan   \n",
       "2        gpt-4o               no+reasoning       0.720±nan    0.737±nan   \n",
       "3        gpt-4o          context+reasoning       0.699±nan    0.699±nan   \n",
       "4        gpt-4o  context+reasoning+complex       0.775±nan    0.807±nan   \n",
       "5   gpt-4o-mini    no+context+no+reasoning       0.747±nan    0.602±nan   \n",
       "6   gpt-4o-mini                 no+context       0.657±nan    0.542±nan   \n",
       "7   gpt-4o-mini               no+reasoning       0.785±nan    0.591±nan   \n",
       "8   gpt-4o-mini          context+reasoning       0.691±nan    0.605±nan   \n",
       "9   gpt-4o-mini  context+reasoning+complex       0.677±nan    0.574±nan   \n",
       "10  deepseek-v3    no+context+no+reasoning       0.766±nan    0.622±nan   \n",
       "11  deepseek-v3                 no+context       0.772±nan    0.598±nan   \n",
       "12  deepseek-v3               no+reasoning       0.679±nan    0.709±nan   \n",
       "13  deepseek-v3          context+reasoning       0.700±nan    0.736±nan   \n",
       "14  deepseek-v3  context+reasoning+complex       0.715±nan    0.753±nan   \n",
       "15   llama-v3p3    no+context+no+reasoning       0.348±nan    0.500±nan   \n",
       "16   llama-v3p3                 no+context       0.856±nan    0.540±nan   \n",
       "17   llama-v3p3               no+reasoning       0.663±nan    0.596±nan   \n",
       "18   llama-v3p3          context+reasoning       0.674±nan    0.705±nan   \n",
       "19   llama-v3p3  context+reasoning+complex       0.755±nan    0.761±nan   \n",
       "20   gemini-2.0    no+context+no+reasoning       0.348±nan    0.500±nan   \n",
       "21   gemini-2.0                 no+context       0.348±nan    0.500±nan   \n",
       "22   gemini-2.0               no+reasoning       0.861±nan    0.560±nan   \n",
       "23   gemini-2.0          context+reasoning       0.621±nan    0.590±nan   \n",
       "24   gemini-2.0  context+reasoning+complex       0.766±nan    0.622±nan   \n",
       "25       yesman                     yesman        0.347561          0.5   \n",
       "26        noman                      noman        0.152439          0.5   \n",
       "27       random                     random             0.5          0.5   \n",
       "\n",
       "     macro_f1  errors  \n",
       "0   0.582±nan     NaN  \n",
       "1   0.634±nan     NaN  \n",
       "2   0.727±nan     NaN  \n",
       "3   0.699±nan     NaN  \n",
       "4   0.784±nan     NaN  \n",
       "5   0.602±nan     NaN  \n",
       "6   0.510±nan     NaN  \n",
       "7   0.582±nan     NaN  \n",
       "8   0.609±nan     NaN  \n",
       "9   0.564±nan     NaN  \n",
       "10  0.629±nan     NaN  \n",
       "11  0.584±nan     NaN  \n",
       "12  0.651±nan     NaN  \n",
       "13  0.685±nan     NaN  \n",
       "14  0.707±nan     NaN  \n",
       "15  0.410±nan     NaN  \n",
       "16  0.490±nan     NaN  \n",
       "17  0.599±nan     NaN  \n",
       "18  0.667±nan     NaN  \n",
       "19  0.758±nan     NaN  \n",
       "20  0.410±nan     NaN  \n",
       "21  0.410±nan     NaN  \n",
       "22  0.526±nan     NaN  \n",
       "23  0.593±nan     NaN  \n",
       "24  0.629±nan     NaN  \n",
       "25   0.410072     0.0  \n",
       "26   0.233645     0.0  \n",
       "27        0.5     0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = []\n",
    "for setting, perf_logs in perf.items():\n",
    "    model = setting.split(\"+\")[0]\n",
    "    context = \"+\".join(setting.split(\"+\")[1:])\n",
    "    avg_perf = pd.DataFrame(perf_logs).mean()\n",
    "    std_pref = pd.DataFrame(perf_logs).std()\n",
    "    results = {\"model\": model, \"context\": context}\n",
    "    for key in avg_perf.keys():\n",
    "        if key != \"errors\":\n",
    "            results[key] = f\"{avg_perf[key]:.3f}±{std_pref[key]:.3f}\"\n",
    "    table.append(results)\n",
    "\n",
    "ground_truth = pd.read_csv(\"ground_truth.csv\")\n",
    "ground_truth = ground_truth[ground_truth.impactful.isin((\"Yes\", \"No\"))]\n",
    "\n",
    "table.append(\n",
    "    {\n",
    "        \"model\": \"yesman\",\n",
    "        \"context\": \"yesman\",\n",
    "        \"errors\": 0,\n",
    "        **evaluate_performance(\n",
    "            ground_truth.impactful, [\"Yes\"] * len(ground_truth), \"Yes\", \"No\"\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "table.append(\n",
    "    {\n",
    "        \"model\": \"noman\",\n",
    "        \"context\": \"noman\",\n",
    "        \"errors\": 0,\n",
    "        **evaluate_performance(\n",
    "            ground_truth.impactful, [\"No\"] * len(ground_truth), \"Yes\", \"No\"\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "table.append(\n",
    "    {\n",
    "        \"model\": \"random\",\n",
    "        \"context\": \"random\",\n",
    "        \"errors\": 0,\n",
    "        \"macro_precision\": 0.5,\n",
    "        \"macro_recall\": 0.5,\n",
    "        \"macro_f1\": 0.5,\n",
    "    }\n",
    ")\n",
    "\n",
    "pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-4o-mini\", \"llama-v3p3\", \"gemini-2.0\"]\n",
    "context = \"context+reasoning\"\n",
    "ground_truth = pd.read_csv(\"ground_truth.csv\")\n",
    "merged = defaultdict(Counter)\n",
    "for repo, dep in zip(ground_truth.repo, ground_truth.dep):\n",
    "    for model in models:\n",
    "        for run in range(10):\n",
    "            file = f\"reports/summary-{model}-{context}-run-{run}.csv\"\n",
    "            df = pd.read_csv(file)\n",
    "            df = df[(df.repo == repo) & (df.dep == dep)]\n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "            merged[(repo, dep)][df.ai_eval.values[0]] += 1\n",
    "merged_perf = []\n",
    "for (repo, dep), counts in merged.items():\n",
    "    merged_perf.append(\n",
    "        {\n",
    "            \"repo\": repo,\n",
    "            \"dep\": dep,\n",
    "            \"ai_yes\": counts[\"Yes\"],\n",
    "            \"ai_no\": counts[\"No\"],\n",
    "            \"ai_majority\": max(counts, key=counts.get),\n",
    "        }\n",
    "    )\n",
    "merged_perf = pd.DataFrame(merged_perf)\n",
    "for repo, dep in zip(merged_perf.repo, merged_perf.dep):\n",
    "    dev_eval = ground_truth[\n",
    "        (ground_truth.repo == repo) & (ground_truth.dep == dep)\n",
    "    ].impactful.values[0]\n",
    "    merged_perf.loc[\n",
    "        (merged_perf.repo == repo) & (merged_perf.dep == dep), \"dev_eval\"\n",
    "    ] = dev_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_precision': 0.7158371040723982,\n",
       " 'macro_recall': 0.6673684210526316,\n",
       " 'macro_f1': 0.6799375487900079}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_performance(merged_perf.dev_eval, merged_perf.ai_majority, \"Yes\", \"No\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abandabot-L22U2UfH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
