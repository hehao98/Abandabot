{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reports/performance.json') as f:\n",
    "    perf = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "context",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "precision",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "recall",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "f1",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "add38dfd-4d6a-43f2-97a7-17090127555d",
       "rows": [
        [
         "0",
         "gpt-4o-mini",
         "no+context+no+reasoning",
         "0.724±0.017",
         "0.782±0.017",
         "0.837±0.022",
         "0.808±0.012"
        ],
        [
         "1",
         "gpt-4o-mini",
         "no+context",
         "0.736±0.018",
         "0.735±0.010",
         "0.972±0.019",
         "0.837±0.012"
        ],
        [
         "2",
         "gpt-4o-mini",
         "no+reasoning",
         "0.753±0.017",
         "0.746±0.014",
         "0.979±0.007",
         "0.846±0.010"
        ],
        [
         "3",
         "gpt-4o-mini",
         "context+reasoning",
         "0.748±0.020",
         "0.753±0.014",
         "0.947±0.018",
         "0.839±0.012"
        ],
        [
         "4",
         "deepseek-v3",
         "no+context+no+reasoning",
         "0.771±0.021",
         "0.832±0.009",
         "0.840±0.028",
         "0.836±0.017"
        ],
        [
         "5",
         "deepseek-v3",
         "no+context",
         "0.762±0.010",
         "0.834±0.005",
         "0.820±0.020",
         "0.827±0.009"
        ],
        [
         "6",
         "deepseek-v3",
         "no+reasoning",
         "0.653±0.011",
         "0.870±0.010",
         "0.589±0.016",
         "0.702±0.012"
        ],
        [
         "7",
         "deepseek-v3",
         "context+reasoning",
         "0.685±0.024",
         "0.896±0.010",
         "0.620±0.032",
         "0.733±0.025"
        ],
        [
         "8",
         "llama-v3p1",
         "no+context+no+reasoning",
         "0.740±0.021",
         "0.737±0.015",
         "0.977±0.012",
         "0.840±0.013"
        ],
        [
         "9",
         "llama-v3p1",
         "no+context",
         "0.595±0.016",
         "0.907±0.034",
         "0.467±0.029",
         "0.615±0.023"
        ],
        [
         "10",
         "llama-v3p1",
         "no+reasoning",
         "0.720±0.011",
         "0.838±0.016",
         "0.740±0.018",
         "0.786±0.009"
        ],
        [
         "11",
         "llama-v3p1",
         "context+reasoning",
         "0.526±0.020",
         "0.882±0.026",
         "0.368±0.026",
         "0.519±0.028"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 12
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>context</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>no+context+no+reasoning</td>\n",
       "      <td>0.724±0.017</td>\n",
       "      <td>0.782±0.017</td>\n",
       "      <td>0.837±0.022</td>\n",
       "      <td>0.808±0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>no+context</td>\n",
       "      <td>0.736±0.018</td>\n",
       "      <td>0.735±0.010</td>\n",
       "      <td>0.972±0.019</td>\n",
       "      <td>0.837±0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>no+reasoning</td>\n",
       "      <td>0.753±0.017</td>\n",
       "      <td>0.746±0.014</td>\n",
       "      <td>0.979±0.007</td>\n",
       "      <td>0.846±0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>context+reasoning</td>\n",
       "      <td>0.748±0.020</td>\n",
       "      <td>0.753±0.014</td>\n",
       "      <td>0.947±0.018</td>\n",
       "      <td>0.839±0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>no+context+no+reasoning</td>\n",
       "      <td>0.771±0.021</td>\n",
       "      <td>0.832±0.009</td>\n",
       "      <td>0.840±0.028</td>\n",
       "      <td>0.836±0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>no+context</td>\n",
       "      <td>0.762±0.010</td>\n",
       "      <td>0.834±0.005</td>\n",
       "      <td>0.820±0.020</td>\n",
       "      <td>0.827±0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>no+reasoning</td>\n",
       "      <td>0.653±0.011</td>\n",
       "      <td>0.870±0.010</td>\n",
       "      <td>0.589±0.016</td>\n",
       "      <td>0.702±0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>context+reasoning</td>\n",
       "      <td>0.685±0.024</td>\n",
       "      <td>0.896±0.010</td>\n",
       "      <td>0.620±0.032</td>\n",
       "      <td>0.733±0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-v3p1</td>\n",
       "      <td>no+context+no+reasoning</td>\n",
       "      <td>0.740±0.021</td>\n",
       "      <td>0.737±0.015</td>\n",
       "      <td>0.977±0.012</td>\n",
       "      <td>0.840±0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-v3p1</td>\n",
       "      <td>no+context</td>\n",
       "      <td>0.595±0.016</td>\n",
       "      <td>0.907±0.034</td>\n",
       "      <td>0.467±0.029</td>\n",
       "      <td>0.615±0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-v3p1</td>\n",
       "      <td>no+reasoning</td>\n",
       "      <td>0.720±0.011</td>\n",
       "      <td>0.838±0.016</td>\n",
       "      <td>0.740±0.018</td>\n",
       "      <td>0.786±0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-v3p1</td>\n",
       "      <td>context+reasoning</td>\n",
       "      <td>0.526±0.020</td>\n",
       "      <td>0.882±0.026</td>\n",
       "      <td>0.368±0.026</td>\n",
       "      <td>0.519±0.028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model                  context     accuracy    precision  \\\n",
       "0   gpt-4o-mini  no+context+no+reasoning  0.724±0.017  0.782±0.017   \n",
       "1   gpt-4o-mini               no+context  0.736±0.018  0.735±0.010   \n",
       "2   gpt-4o-mini             no+reasoning  0.753±0.017  0.746±0.014   \n",
       "3   gpt-4o-mini        context+reasoning  0.748±0.020  0.753±0.014   \n",
       "4   deepseek-v3  no+context+no+reasoning  0.771±0.021  0.832±0.009   \n",
       "5   deepseek-v3               no+context  0.762±0.010  0.834±0.005   \n",
       "6   deepseek-v3             no+reasoning  0.653±0.011  0.870±0.010   \n",
       "7   deepseek-v3        context+reasoning  0.685±0.024  0.896±0.010   \n",
       "8    llama-v3p1  no+context+no+reasoning  0.740±0.021  0.737±0.015   \n",
       "9    llama-v3p1               no+context  0.595±0.016  0.907±0.034   \n",
       "10   llama-v3p1             no+reasoning  0.720±0.011  0.838±0.016   \n",
       "11   llama-v3p1        context+reasoning  0.526±0.020  0.882±0.026   \n",
       "\n",
       "         recall           f1  \n",
       "0   0.837±0.022  0.808±0.012  \n",
       "1   0.972±0.019  0.837±0.012  \n",
       "2   0.979±0.007  0.846±0.010  \n",
       "3   0.947±0.018  0.839±0.012  \n",
       "4   0.840±0.028  0.836±0.017  \n",
       "5   0.820±0.020  0.827±0.009  \n",
       "6   0.589±0.016  0.702±0.012  \n",
       "7   0.620±0.032  0.733±0.025  \n",
       "8   0.977±0.012  0.840±0.013  \n",
       "9   0.467±0.029  0.615±0.023  \n",
       "10  0.740±0.018  0.786±0.009  \n",
       "11  0.368±0.026  0.519±0.028  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = []\n",
    "for setting, perf_logs in perf.items():\n",
    "    model = setting.split(\"+\")[0]\n",
    "    context = \"+\".join(setting.split(\"+\")[1:])\n",
    "    avg_perf = pd.DataFrame(perf_logs).mean()\n",
    "    std_pref = pd.DataFrame(perf_logs).std()\n",
    "    results = {\"model\": model, \"context\": context}\n",
    "    for key in avg_perf.keys():\n",
    "        results[key] = f\"{avg_perf[key]:.3f}±{std_pref[key]:.3f}\"\n",
    "    table.append(results)\n",
    "pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-4o-mini\", \"deepseek-v3\"]\n",
    "context = \"no+context+no+reasoning\"\n",
    "ground_truth = pd.read_csv(\"ground_truth.csv\")\n",
    "merged = defaultdict(Counter)\n",
    "for repo, dep in zip(ground_truth.repo, ground_truth.dep):\n",
    "    for model in models:\n",
    "        for run in range(10):\n",
    "            file = f\"reports/summary-{model}-{context}-run-{run}.csv\"\n",
    "            df = pd.read_csv(file)\n",
    "            df = df[(df.repo == repo) & (df.dep == dep)]\n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "            merged[(repo, dep)][df.ai_eval.values[0]] += 1\n",
    "merged_perf = []\n",
    "for (repo, dep), counts in merged.items():\n",
    "    merged_perf.append(\n",
    "        {\n",
    "            \"repo\": repo,\n",
    "            \"dep\": dep,\n",
    "            \"ai_yes\": counts[\"Yes\"],\n",
    "            \"ai_no\": counts[\"No\"],\n",
    "            \"ai_majority\": max(counts, key=counts.get),\n",
    "        }\n",
    "    )\n",
    "merged_perf = pd.DataFrame(merged_perf)\n",
    "for repo, dep in zip(merged_perf.repo, merged_perf.dep):\n",
    "    dev_eval = ground_truth[\n",
    "        (ground_truth.repo == repo) & (ground_truth.dep == dep)\n",
    "    ].impactful.values[0]\n",
    "    merged_perf.loc[\n",
    "        (merged_perf.repo == repo) & (merged_perf.dep == dep), \"dev_eval\"\n",
    "    ] = dev_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7804878048780488,\n",
       " 'precision': 0.819672131147541,\n",
       " 'recall': 0.8771929824561403,\n",
       " 'f1': 0.8474576271186439}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_performance(\n",
    "    ground_truth: pd.Series, report: pd.Series, true_label: str, false_label: str\n",
    ") -> dict[str, float]:\n",
    "    total, tp, fp, tn, fn = len(report), 0, 0, 0, 0\n",
    "    for dev_eval, ai_eval in zip(ground_truth, report):\n",
    "        if dev_eval == true_label and ai_eval == true_label:\n",
    "            tp += 1\n",
    "        elif dev_eval == false_label and ai_eval == true_label:\n",
    "            fp += 1\n",
    "        elif dev_eval == true_label and ai_eval == false_label:\n",
    "            fn += 1\n",
    "        elif dev_eval == false_label and ai_eval == false_label:\n",
    "            tn += 1\n",
    "    acc, precision, recall = (tp + tn) / (total), tp / (tp + fp), tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "evaluate_performance(merged_perf.dev_eval, merged_perf.ai_majority, \"Yes\", \"No\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abandabot-L22U2UfH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
